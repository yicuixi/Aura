import os
import json
from datetime import datetime
import requests

"""
Aura AI - è½»é‡çº§ç‰ˆæœ¬
ç›´æ¥è°ƒç”¨Ollama APIï¼Œä¸ä½¿ç”¨LangChainæ¡†æ¶
"""

# ç®€å•çš„è®°å¿†ç³»ç»Ÿ
class Memory:
    def __init__(self, file_path="memory.json"):
        self.file_path = file_path
        self.data = self._load()
    
    def _load(self):
        if os.path.exists(self.file_path):
            try:
                with open(self.file_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return {"conversations": [], "facts": {}}
        return {"conversations": [], "facts": {}}
    
    def save(self):
        with open(self.file_path, 'w', encoding='utf-8') as f:
            json.dump(self.data, f, ensure_ascii=False, indent=2)
    
    def add_conversation(self, user_input, response):
        self.data["conversations"].append({
            "timestamp": datetime.now().isoformat(),
            "user": user_input,
            "assistant": response
        })
        self.save()
    
    def add_fact(self, key, value):
        self.data["facts"][key] = {
            "value": value,
            "timestamp": datetime.now().isoformat()
        }
        self.save()
    
    def get_fact(self, key):
        return self.data["facts"].get(key, {}).get("value")
    
    def get_recent_conversations(self, limit=5):
        return self.data["conversations"][-limit:]

# Ollama API å®¢æˆ·ç«¯
class OllamaClient:
    def __init__(self, base_url="http://localhost:11435", model="qwen2"):
        self.base_url = base_url
        self.model = model
    
    def generate(self, prompt, system=""):
        try:
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "system": system,
                    "stream": False
                }
            )
            return response.json()["response"]
        except Exception as e:
            return f"ç”Ÿæˆå¤±è´¥: {str(e)}"

# ä¸»ç¨‹åº
def main():
    print("âœ¨ æ­£åœ¨åˆå§‹åŒ–Aura...")
    
    # åˆå§‹åŒ–ç»„ä»¶
    memory = Memory()
    ollama = OllamaClient(model="qwen2")  # æˆ– qwen2.5:7b
    
    # ç³»ç»Ÿæç¤º
    system_prompt = """ä½ æ˜¯Auraï¼Œä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚ä½ åº”è¯¥å‹å¥½ã€ä¹äºåŠ©äººã€è¯šå®ã€‚
    ä½ æ˜¯Lydiaçš„ä¸ªäººåŠ©æ‰‹ï¼Œå¥¹æ˜¯ä¸€åå…‰å­¦ç ”äºŒç¡•å£«ç”Ÿï¼Œç ”ç©¶æ–¹å‘æ˜¯OAMç›¸ä½é‡å»º+å°‘æ ·æœ¬è¯†åˆ«ã€‚
    å¥¹æ­£åœ¨å‡†å¤‡è‹±ä¼Ÿè¾¾çš„Deep Learning Software Test Engineer InternèŒä½çš„é¢è¯•ã€‚
    """
    
    print("âœ¨ Auraå·²å¯åŠ¨ï¼Œç­‰å¾…æŒ‡ä»¤...")
    
    while True:
        user_input = input("\nğŸ‘¤ è¾“å…¥: ")
        if user_input.lower() in ["exit", "quit", "é€€å‡º"]:
            print("ğŸ‘‹ Auraæ­£åœ¨å…³é—­...")
            break
        
        # è·å–æœ€è¿‘å¯¹è¯
        recent = memory.get_recent_conversations()
        conversation_context = "\\n".join([
            f"ç”¨æˆ·: {conv['user']}\\nåŠ©æ‰‹: {conv['assistant']}" 
            for conv in recent
        ])
        
        # æ„å»ºå®Œæ•´æç¤º
        full_prompt = f"{conversation_context}\\nç”¨æˆ·: {user_input}\\nåŠ©æ‰‹:"
        
        # ç”Ÿæˆå›å¤
        response = ollama.generate(full_prompt, system_prompt)
        print(f"\nğŸ¤– Aura: {response}")
        
        # è®°å¿†å¯¹è¯
        memory.add_conversation(user_input, response)

if __name__ == "__main__":
    main()
