"""
Aura AI - ç»ˆæä¿®å¤ç‰ˆæœ¬ï¼Œå½»åº•è§£å†³æ¨¡å‹è¾“å‡ºè§£æé—®é¢˜
ä¸»è¦ä¿®å¤ï¼š
1. æç®€åŒ–Agentï¼Œé¿å…å¤æ‚çš„ReActè§£æ
2. ç›´æ¥ä½¿ç”¨LLMï¼Œç»•è¿‡LangChainçš„Agentè§£æå™¨
3. æ‰‹åŠ¨å®ç°å·¥å…·è°ƒç”¨é€»è¾‘
4. å¢å¼ºè¾“å‡ºæ¸…ç†æœºåˆ¶
"""

import os
import json
import re
from datetime import datetime
import logging
from typing import Dict, List, Any, Union, Optional

from langchain_community.llms import Ollama
from langchain.chains import RetrievalQA

from rag import RAGSystem
import tools
from memory import LongTermMemory

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("aura.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("AuraUltimate")

class UltraCleanOllama(Ollama):
    """ç»ˆæå¢å¼ºçš„OllamaåŒ…è£…å™¨ï¼Œå½»åº•æ¸…ç†è¾“å‡º"""
    
    def _call(self, prompt: str, stop=None, run_manager=None, **kwargs):
        """é‡å†™è°ƒç”¨æ–¹æ³•ï¼Œå½»åº•æ¸…ç†è¾“å‡º"""
        try:
            raw_output = super()._call(prompt, stop, run_manager, **kwargs)
            cleaned = self._ultra_clean_output(raw_output)
            logger.debug(f"åŸå§‹è¾“å‡º: {raw_output[:100]}...")
            logger.debug(f"æ¸…ç†åè¾“å‡º: {cleaned[:100]}...")
            return cleaned
        except Exception as e:
            logger.error(f"LLMè°ƒç”¨é”™è¯¯: {e}")
            return "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚"
    
    def _ultra_clean_output(self, output: str) -> str:
        """ç»ˆææ¸…ç†è¾“å‡º"""
        if not output or not isinstance(output, str):
            return "æˆ‘ç†è§£äº†æ‚¨çš„é—®é¢˜ï¼Œè®©æˆ‘ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚"
        
        # ç§»é™¤æ‰€æœ‰å¯èƒ½çš„æ€è€ƒæ ‡ç­¾ï¼ˆè´ªå©ªåŒ¹é…ï¼‰
        patterns_to_remove = [
            r'<think>.*?</think>',
            r'<thinking>.*?</thinking>',
            r'<reasoning>.*?</reasoning>',
            r'<analysis>.*?</analysis>',
            r'<reflection>.*?</reflection>',
            r'<è€ƒè™‘>.*?</è€ƒè™‘>',
            r'<æ€è€ƒ>.*?</æ€è€ƒ>',
            # ç§»é™¤ä¸å®Œæ•´çš„æ ‡ç­¾
            r'<think>.*',
            r'<thinking>.*',
            r'<reasoning>.*',
            r'.*</think>',
            r'.*</thinking>',
            r'.*</reasoning>',
        ]
        
        cleaned = output
        for pattern in patterns_to_remove:
            cleaned = re.sub(pattern, '', cleaned, flags=re.DOTALL | re.IGNORECASE)
        
        # ç§»é™¤LangChainç›¸å…³çš„æ ¼å¼æ ‡è®°
        langchain_patterns = [
            r'Thought:.*?(?=Action:|Final Answer:|$)',
            r'Action:.*?(?=Action Input:|Observation:|Final Answer:|$)',
            r'Action Input:.*?(?=Observation:|Final Answer:|$)',
            r'Observation:.*?(?=Thought:|Final Answer:|$)',
            r'Final Answer:\s*',
        ]
        
        for pattern in langchain_patterns:
            cleaned = re.sub(pattern, '', cleaned, flags=re.DOTALL | re.IGNORECASE)
        
        # æ¸…ç†å¤šä½™çš„ç©ºç™½
        cleaned = re.sub(r'\n\s*\n', '\n', cleaned)
        cleaned = re.sub(r'\s+', ' ', cleaned)
        cleaned = cleaned.strip()
        
        # ç§»é™¤å¯èƒ½çš„å¼•å·åŒ…å›´
        if cleaned.startswith('`') and cleaned.endswith('`'):
            cleaned = cleaned[1:-1].strip()
        
        # å¦‚æœæ¸…ç†åå†…å®¹ä¸ºç©ºæˆ–å¤ªçŸ­ï¼Œè¿”å›é»˜è®¤å“åº”
        if not cleaned or len(cleaned) < 5:
            cleaned = "æˆ‘ç†è§£äº†æ‚¨çš„é—®é¢˜ï¼Œè®©æˆ‘ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚"
        
        return cleaned

class AuraAgentUltimate:
    """AuraåŠ©æ‰‹çš„ç»ˆæä¿®å¤ç‰ˆAgentç±» - ä¸ä½¿ç”¨LangChain Agent"""
    
    def __init__(self, model_name="qwen3:4b", verbose=True):
        """åˆå§‹åŒ–Aura Agent"""
        logger.info("åˆå§‹åŒ–Aura Agent (ç»ˆæç‰ˆ)...")
        
        # é…ç½®
        self.model_name = model_name
        self.verbose = verbose
        
        # åˆå§‹åŒ–é•¿æœŸè®°å¿†
        self.long_term_memory = LongTermMemory()
        logger.info("é•¿æœŸè®°å¿†ç³»ç»Ÿå·²åˆå§‹åŒ–")
        
        # åˆå§‹åŒ–æ¨¡å‹ - ä½¿ç”¨æç®€çš„ç³»ç»Ÿæç¤º
        self.llm = UltraCleanOllama(
            model=model_name, 
            base_url="http://localhost:11434",
            system=self._get_ultra_simple_system_prompt(),
            temperature=0.7
        )
        logger.info(f"å·²è¿æ¥åˆ°Ollamaæ¨¡å‹: {model_name}")
        
        # åˆå§‹åŒ–RAGç³»ç»Ÿ
        self._init_rag_system()
        
        # åˆå§‹åŒ–å·¥å…·å­—å…¸
        self._init_tools_dict()
        
        logger.info("Aura Agentç»ˆæç‰ˆåˆå§‹åŒ–å®Œæˆ")
    
    def _get_ultra_simple_system_prompt(self) -> str:
        """è·å–æç®€ç³»ç»Ÿæç¤ºè¯"""
        return """ä½ æ˜¯Auraï¼Œç”¨æˆ·çš„AIåŠ©æ‰‹ã€‚

ä¸¥æ ¼éµå®ˆä»¥ä¸‹è§„åˆ™ï¼š
1. ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œä¸è¦ä½¿ç”¨ä»»ä½•XMLæ ‡ç­¾
2. ä¸è¦è¾“å‡ºæ€è€ƒè¿‡ç¨‹ï¼Œåªè¾“å‡ºæœ€ç»ˆç­”æ¡ˆ
3. ä¿æŒç®€æ´å‹å¥½çš„è¯­è°ƒ
4. å¦‚æœéœ€è¦å·¥å…·å¸®åŠ©ï¼Œæˆ‘ä¼šå•ç‹¬è°ƒç”¨

ä½ çš„èº«ä»½ï¼šæ™ºèƒ½åŠ©æ‰‹Auraï¼Œèƒ½å¤Ÿå›ç­”é—®é¢˜ã€è®°å¿†ä¿¡æ¯ã€æœç´¢ç½‘ç»œã€‚

è¯·ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œä¸è¦æœ‰ä»»ä½•å¤šä½™çš„æ ‡è®°æˆ–æ ¼å¼ã€‚"""
    
    def _init_rag_system(self):
        """åˆå§‹åŒ–RAGçŸ¥è¯†æ£€ç´¢ç³»ç»Ÿ"""
        if not os.path.exists("db"):
            os.makedirs("db")
            
        self.rag_system = RAGSystem(persist_directory="db")
        logger.info("RAGç³»ç»Ÿå·²åˆå§‹åŒ–")
        
        self.retrieval_qa = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.rag_system.vectorstore.as_retriever(),
            return_source_documents=True
        )
    
    def _init_tools_dict(self):
        """åˆå§‹åŒ–å·¥å…·å­—å…¸"""
        self.tools_dict = {
            "search_web": tools.search_web,
            "read_file": tools.read_file,
            "write_file": tools.write_file,
            "list_directory": tools.list_directory,
            "search_knowledge": self.search_knowledge,
            "remember_fact": self.remember_fact,
            "recall_fact": self.recall_fact
        }
        logger.info(f"å·²åˆå§‹åŒ– {len(self.tools_dict)} ä¸ªå·¥å…·")
    
    def search_knowledge(self, query: str) -> str:
        """æœç´¢æœ¬åœ°çŸ¥è¯†åº“"""
        try:
            logger.info(f"æ­£åœ¨æœç´¢çŸ¥è¯†åº“: {query}")
            result = self.retrieval_qa({"query": query})
            return result["result"]
        except Exception as e:
            logger.error(f"çŸ¥è¯†åº“æœç´¢é”™è¯¯: {str(e)}")
            return f"çŸ¥è¯†åº“æœç´¢å‡ºé”™: {str(e)}"
    
    def remember_fact(self, fact_str: str) -> str:
        """è®°ä½ä¸€ä¸ªäº‹å®"""
        try:
            parts = fact_str.split('/')
            if len(parts) != 3:
                return "æ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨: category/key/value"
            
            category, key, value = parts
            self.long_term_memory.add_fact(category, key, value)
            logger.info(f"è®°å¿†å·²æ·»åŠ : {category}/{key}/{value}")
            return f"å·²è®°ä½: {category}/{key}/{value}"
        except Exception as e:
            logger.error(f"æ·»åŠ è®°å¿†é”™è¯¯: {str(e)}")
            return f"è®°å¿†å‡ºé”™: {str(e)}"
    
    def recall_fact(self, fact_str: str) -> str:
        """å›å¿†ä¸€ä¸ªäº‹å®"""
        try:
            parts = fact_str.split('/')
            if len(parts) != 2:
                return "æ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨: category/key"
                
            category, key = parts
            value = self.long_term_memory.get_fact(category, key)
            
            if value is None:
                return f"æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è®°å¿†: {category}/{key}"
                
            logger.info(f"è®°å¿†å·²æ£€ç´¢: {category}/{key}")
            return f"{category}/{key}: {value}"
        except Exception as e:
            logger.error(f"æ£€ç´¢è®°å¿†é”™è¯¯: {str(e)}")
            return f"å›å¿†å‡ºé”™: {str(e)}"
    
    def _should_use_tool(self, query: str) -> tuple[bool, str]:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦ä½¿ç”¨å·¥å…·"""
        query_lower = query.lower()
        
        # å®æ—¶ä¿¡æ¯æŸ¥è¯¢
        realtime_keywords = ["å¤©æ°”", "weather", "æ–°é—»", "news", "è‚¡ç¥¨", "ä»Šå¤©", "ç°åœ¨", "æœ€æ–°"]
        if any(keyword in query_lower for keyword in realtime_keywords):
            return True, "search_web"
        
        # çŸ¥è¯†åº“æŸ¥è¯¢
        knowledge_keywords = ["ä»€ä¹ˆæ˜¯", "è§£é‡Š", "åŸç†", "å¦‚ä½•", "æŠ€æœ¯", "ç®—æ³•", "æ¨¡å‹"]
        if any(keyword in query_lower for keyword in knowledge_keywords):
            return True, "search_knowledge"
        
        # è®°å¿†æ“ä½œ
        if "è®°ä½" in query or "remember" in query_lower:
            return True, "remember_fact"
        
        if "å›å¿†" in query or "è®°å¾—" in query or "recall" in query_lower:
            return True, "recall_fact"
        
        # æ–‡ä»¶æ“ä½œ
        if "è¯»å–æ–‡ä»¶" in query or "read file" in query_lower:
            return True, "read_file"
        
        if "å†™å…¥æ–‡ä»¶" in query or "write file" in query_lower:
            return True, "write_file"
        
        return False, ""
    
    def _extract_tool_input(self, query: str, tool_name: str) -> str:
        """ä»æŸ¥è¯¢ä¸­æå–å·¥å…·è¾“å…¥"""
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´æ™ºèƒ½çš„å‚æ•°æå–é€»è¾‘
        if tool_name == "search_web":
            return query
        elif tool_name == "search_knowledge":
            return query
        elif tool_name == "remember_fact":
            # æå–è¦è®°ä½çš„ä¿¡æ¯
            if "è®°ä½" in query:
                parts = query.split("è®°ä½")
                if len(parts) > 1:
                    content = parts[1].strip()
                    # ç®€å•æ ¼å¼åŒ–ä¸º category/key/value
                    return f"user/preference/{content}"
            return query
        elif tool_name == "recall_fact":
            # æå–è¦å›å¿†çš„ä¿¡æ¯
            return "user/preference"  # é»˜è®¤æŸ¥è¯¢ç”¨æˆ·åå¥½
        
        return query
    
    def process_query(self, query: str) -> str:
        """å¤„ç†ç”¨æˆ·æŸ¥è¯¢ - ç»ˆæä¿®å¤ç‰ˆ"""
        try:
            logger.info(f"å¤„ç†ç”¨æˆ·æŸ¥è¯¢: {query}")
            
            # é¦–å…ˆåˆ¤æ–­æ˜¯å¦éœ€è¦å·¥å…·
            need_tool, tool_name = self._should_use_tool(query)
            
            if need_tool and tool_name in self.tools_dict:
                logger.info(f"ä½¿ç”¨å·¥å…·: {tool_name}")
                try:
                    tool_input = self._extract_tool_input(query, tool_name)
                    tool_result = self.tools_dict[tool_name](tool_input)
                    
                    # åŸºäºå·¥å…·ç»“æœç”Ÿæˆå›ç­”
                    context_prompt = f"""åŸºäºä»¥ä¸‹å·¥å…·æŸ¥è¯¢ç»“æœå›ç­”ç”¨æˆ·é—®é¢˜ï¼š

ç”¨æˆ·é—®é¢˜ï¼š{query}
å·¥å…·ç»“æœï¼š{tool_result}

è¯·ç›´æ¥ç»™å‡ºç®€æ´çš„å›ç­”ï¼Œä¸è¦ä½¿ç”¨ä»»ä½•XMLæ ‡ç­¾æˆ–æ ¼å¼æ ‡è®°ï¼š"""
                    
                    response = self.llm.invoke(context_prompt)
                    
                except Exception as tool_error:
                    logger.error(f"å·¥å…·è°ƒç”¨é”™è¯¯: {tool_error}")
                    # å¦‚æœå·¥å…·å¤±è´¥ï¼Œç›´æ¥ç”¨LLMå›ç­”
                    simple_prompt = f"è¯·ç®€æ´åœ°å›ç­”ï¼š{query}"
                    response = self.llm.invoke(simple_prompt)
            else:
                # ç›´æ¥ç”¨LLMå›ç­”
                simple_prompt = f"è¯·ç®€æ´åœ°å›ç­”ï¼š{query}"
                response = self.llm.invoke(simple_prompt)
            
            # ä¿å­˜å¯¹è¯å†å²
            self.long_term_memory.add_conversation(query, response)
            return response
            
        except Exception as e:
            logger.error(f"å¤„ç†æŸ¥è¯¢å‡ºé”™: {str(e)}")
            return "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶é‡åˆ°äº†é”™è¯¯ã€‚è¯·å°è¯•é‡æ–°è¡¨è¿°æ‚¨çš„é—®é¢˜ã€‚"
    
    def load_knowledge(self, extension=".md") -> str:
        """åŠ è½½çŸ¥è¯†åˆ°RAGç³»ç»Ÿ"""
        try:
            data_dir = os.path.join(os.getcwd(), "data")
            logger.info(f"ä»{data_dir}åŠ è½½{extension}æ ¼å¼çš„çŸ¥è¯†æ–‡æ¡£")
            
            if not os.path.exists(data_dir):
                return f"âŒ dataç›®å½•ä¸å­˜åœ¨: {data_dir}"
                
            files = os.listdir(data_dir)
            target_files = [f for f in files if f.endswith(extension)]
            
            if not target_files:
                return f"âŒ dataç›®å½•ä¸­æ²¡æœ‰{extension}æ ¼å¼çš„æ–‡ä»¶"
                
            # æ‰‹åŠ¨åŠ è½½æ¯ä¸ªç›®æ ‡æ–‡ä»¶
            documents = []
            for file_name in target_files:
                file_path = os.path.join(data_dir, file_name)
                logger.info(f"åŠ è½½æ–‡ä»¶: {file_path}")
                
                try:
                    if extension == ".pdf":
                        from langchain.document_loaders import PyPDFLoader
                        loader = PyPDFLoader(file_path)
                    elif extension == ".csv":
                        from langchain.document_loaders import CSVLoader
                        loader = CSVLoader(file_path)
                    else:  # .mdæˆ–å…¶ä»–æ–‡ä»¶
                        from langchain.document_loaders import TextLoader
                        loader = TextLoader(file_path, encoding='utf-8')
                        
                    file_docs = loader.load()
                    documents.extend(file_docs)
                    logger.info(f"æˆåŠŸåŠ è½½ {file_path}, åŒ…å« {len(file_docs)} ä¸ªæ–‡æ¡£")
                except Exception as e:
                    logger.error(f"åŠ è½½æ–‡ä»¶ {file_path} å‡ºé”™: {str(e)}")
            
            if not documents:
                return "âŒ æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ–‡æ¡£"
                
            # æ–‡æœ¬åˆ†å‰²
            from langchain.text_splitter import RecursiveCharacterTextSplitter
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=500,
                chunk_overlap=50
            )
            splits = text_splitter.split_documents(documents)
            logger.info(f"åˆ›å»ºäº† {len(splits)} ä¸ªæ–‡æœ¬å—")
            
            # æ·»åŠ åˆ°å‘é‡åº“
            self.rag_system.vectorstore.add_documents(splits)
            logger.info("æ–‡æ¡£å·²æ·»åŠ åˆ°çŸ¥è¯†åº“")
            
            # æŒä¹…åŒ–ä¿å­˜
            self.rag_system.vectorstore.persist()
            logger.info("çŸ¥è¯†åº“å·²æŒä¹…åŒ–ä¿å­˜")
            
            return "âœ… çŸ¥è¯†åº“å·²æ›´æ–°"
        except Exception as e:
            logger.error(f"åŠ è½½çŸ¥è¯†å‡ºé”™: {str(e)}")
            return f"âŒ åŠ è½½çŸ¥è¯†å‡ºé”™: {str(e)}"
    
    def run_cli(self):
        """è¿è¡Œå‘½ä»¤è¡Œç•Œé¢"""
        print("=" * 50)
        print("âœ¨ Auraå·²å¯åŠ¨ (ç»ˆæä¿®å¤ç‰ˆ)ï¼Œå½»åº•è§£å†³è§£æé—®é¢˜")
        print("ğŸ’¡ æç¤º: ä½¿ç”¨'exit'æˆ–'é€€å‡º'å¯ä»¥ç»“æŸå¯¹è¯")
        print("ğŸ’¡ ç‰¹æ®Šå‘½ä»¤: 'åŠ è½½çŸ¥è¯†' - åŠ è½½dataç›®å½•ä¸­çš„æ–‡æ¡£åˆ°çŸ¥è¯†åº“")
        print("=" * 50)
        
        while True:
            user_input = input("\nğŸ‘¤ è¾“å…¥: ")
            
            # é€€å‡ºå‘½ä»¤
            if user_input.lower() in ["exit", "quit", "é€€å‡º"]:
                print("ğŸ‘‹ Auraæ­£åœ¨å…³é—­...")
                break
            
            # ç‰¹æ®Šå‘½ä»¤å¤„ç†
            if user_input.lower() == "åŠ è½½çŸ¥è¯†":
                extension = input("è¯·è¾“å…¥æ–‡ä»¶æ‰©å±•å(é»˜è®¤.md): ") or ".md"
                result = self.load_knowledge(extension)
                print(result)
                continue
                
            # å¤„ç†ä¸€èˆ¬æŸ¥è¯¢
            response = self.process_query(user_input)
            print(f"\nğŸ¤– Aura: {response}")

def main():
    """ä¸»ç¨‹åºå…¥å£"""
    try:
        # åˆ›å»ºç»ˆæä¿®å¤ç‰ˆAura Agent
        aura = AuraAgentUltimate(model_name="qwen3:4b", verbose=False)
        
        # è¿è¡Œå‘½ä»¤è¡Œç•Œé¢
        aura.run_cli()
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ç¨‹åºå·²é€€å‡º")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå¯åŠ¨å¤±è´¥: {str(e)}")
        print("è¯·æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦æ­£åœ¨è¿è¡Œï¼Œä»¥åŠæ¨¡å‹æ˜¯å¦å·²ä¸‹è½½")

if __name__ == "__main__":
    main()
