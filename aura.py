"""
Aura AI - ä¿®å¤ç‰ˆæœ¬ï¼Œå½»åº•è§£å†³thinkæ ‡ç­¾é—®é¢˜
ä¸»è¦ä¿®å¤ï¼š
1. ä¿®å¤æ‹¼å†™é”™è¯¯ï¼šloggingjie -> logging
2. è¶…å¼ºè¾“å‡ºæ¸…ç†æœºåˆ¶ï¼Œå½»åº•ç§»é™¤æ‰€æœ‰æ€è€ƒæ ‡è®°
3. ç»•è¿‡LangChain Agentè§£æå™¨ï¼Œç›´æ¥å¤„ç†è¾“å‡º
4. å¤šå±‚é˜²æŠ¤ï¼Œç¡®ä¿ç»ä¸è¾“å‡ºthinkæ ‡ç­¾
"""

import os
import json
import re
from datetime import datetime
import logging
from typing import Dict, List, Any, Union, Optional

from langchain_community.llms import Ollama
from langchain.chains import RetrievalQA

from rag import RAGSystem
import tools
from memory import LongTermMemory

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("aura.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("AuraFixed")

class SuperCleanOllama(Ollama):
    """è¶…çº§æ¸…ç†çš„OllamaåŒ…è£…å™¨ï¼Œå½»åº•ç¦æ­¢thinkæ ‡ç­¾"""
    
    def _call(self, prompt: str, stop=None, run_manager=None, **kwargs):
        """é‡å†™è°ƒç”¨æ–¹æ³•ï¼Œè¶…å¼ºæ¸…ç†è¾“å‡º"""
        try:
            raw_output = super()._call(prompt, stop, run_manager, **kwargs)
            cleaned = self._super_clean_output(raw_output)
            logger.debug(f"åŸå§‹è¾“å‡º: {raw_output[:200]}...")
            logger.debug(f"æ¸…ç†åè¾“å‡º: {cleaned[:200]}...")
            return cleaned
        except Exception as e:
            logger.error(f"LLMè°ƒç”¨é”™è¯¯: {e}")
            return "æˆ‘ç†è§£äº†æ‚¨çš„é—®é¢˜ï¼Œè®©æˆ‘ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚"
    
    def _super_clean_output(self, output: str) -> str:
        """è¶…çº§æ¸…ç†è¾“å‡ºï¼Œå½»åº•ç§»é™¤æ‰€æœ‰æ€è€ƒæ ‡è®°"""
        if not output or not isinstance(output, str):
            return "æˆ‘ç†è§£äº†æ‚¨çš„é—®é¢˜ï¼Œè®©æˆ‘ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚"
        
        # ç¬¬ä¸€è½®ï¼šç§»é™¤æ‰€æœ‰å¯èƒ½çš„æ€è€ƒæ ‡ç­¾ï¼ˆè¶…çº§è´ªå©ªåŒ¹é…ï¼‰
        think_patterns = [
            r'<think>.*?</think>',
            r'<thinking>.*?</thinking>',
            r'<reasoning>.*?</reasoning>',
            r'<analysis>.*?</analysis>',
            r'<reflection>.*?</reflection>',
            r'<thoughts>.*?</thoughts>',
            r'<consider>.*?</consider>',
            r'<evaluate>.*?</evaluate>',
            r'<process>.*?</process>',
            r'<è€ƒè™‘>.*?</è€ƒè™‘>',
            r'<æ€è€ƒ>.*?</æ€è€ƒ>',
            r'<åˆ†æ>.*?</åˆ†æ>',
            r'<æ¨ç†>.*?</æ¨ç†>',
            # ç§»é™¤ä¸å®Œæ•´çš„æ ‡ç­¾
            r'<think>.*',
            r'.*</think>',
            r'<thinking>.*',
            r'.*</thinking>',
            r'<reasoning>.*',
            r'.*</reasoning>',
            # ç§»é™¤ä»¥thinkå¼€å¤´çš„ä»»ä½•å†…å®¹
            r'think.*?(?=\n|\.|ã€‚|ï¼|ï¼Ÿ|!|\?)',
            r'thinking.*?(?=\n|\.|ã€‚|ï¼|ï¼Ÿ|!|\?)',
        ]
        
        cleaned = output
        for pattern in think_patterns:
            cleaned = re.sub(pattern, '', cleaned, flags=re.DOTALL | re.IGNORECASE)
        
        # ç¬¬äºŒè½®ï¼šç§»é™¤LangChainç›¸å…³çš„æ ¼å¼æ ‡è®°
        langchain_patterns = [
            r'Thought:.*?(?=Action:|Final Answer:|$)',
            r'Action:.*?(?=Action Input:|Observation:|Final Answer:|$)',
            r'Action Input:.*?(?=Observation:|Final Answer:|$)',
            r'Observation:.*?(?=Thought:|Final Answer:|$)',
            r'Final Answer:\s*',
        ]
        
        for pattern in langchain_patterns:
            cleaned = re.sub(pattern, '', cleaned, flags=re.DOTALL | re.IGNORECASE)
        
        # ç¬¬ä¸‰è½®ï¼šç§»é™¤åå¼•å·åŒ…å›´çš„å†…å®¹ï¼ˆé€šå¸¸åŒ…å«æ€è€ƒè¿‡ç¨‹ï¼‰
        if cleaned.startswith('`') and cleaned.endswith('`'):
            cleaned = cleaned[1:-1].strip()
        
        # ç¬¬å››è½®ï¼šæ¸…ç†å¤šä½™çš„ç©ºç™½
        cleaned = re.sub(r'\n\s*\n', '\n', cleaned)
        cleaned = re.sub(r'\s+', ' ', cleaned)
        cleaned = cleaned.strip()
        
        # ç¬¬äº”è½®ï¼šå¦‚æœä»ç„¶åŒ…å«thinkç›¸å…³å†…å®¹ï¼Œå¼ºåˆ¶æ›¿æ¢
        if any(word in cleaned.lower() for word in ['<think', 'think>', 'thinking', '<reasoning']):
            logger.warning("æ£€æµ‹åˆ°æ®‹ç•™çš„æ€è€ƒæ ‡è®°ï¼Œå¼ºåˆ¶æ¸…ç†")
            cleaned = re.sub(r'.*think.*', '', cleaned, flags=re.IGNORECASE)
            cleaned = re.sub(r'.*reasoning.*', '', cleaned, flags=re.IGNORECASE)
            cleaned = cleaned.strip()
        
        # æœ€åæ£€æŸ¥ï¼šå¦‚æœæ¸…ç†åå†…å®¹ä¸ºç©ºæˆ–å¤ªçŸ­ï¼Œè¿”å›é»˜è®¤å“åº”
        if not cleaned or len(cleaned) < 10:
            cleaned = "æˆ‘ç†è§£äº†æ‚¨çš„é—®é¢˜ï¼Œè®©æˆ‘ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚"
        
        return cleaned

class AuraAgentFixed:
    """AuraåŠ©æ‰‹çš„ä¿®å¤ç‰ˆAgentç±» - å½»åº•è§£å†³thinkæ ‡ç­¾é—®é¢˜"""
    
    def __init__(self, model_name="qwen3:4b", verbose=True):
        """åˆå§‹åŒ–Aura Agent"""
        logger.info("åˆå§‹åŒ–Aura Agent (ä¿®å¤ç‰ˆ)...")
        
        # é…ç½®
        self.model_name = model_name
        self.verbose = verbose
        
        # åˆå§‹åŒ–é•¿æœŸè®°å¿†
        self.long_term_memory = LongTermMemory()
        logger.info("é•¿æœŸè®°å¿†ç³»ç»Ÿå·²åˆå§‹åŒ–")
        
        # åˆå§‹åŒ–æ¨¡å‹ - ä½¿ç”¨è¶…å¼ºåˆ¶çš„ç³»ç»Ÿæç¤º
        self.llm = SuperCleanOllama(
            model=model_name, 
            base_url="http://localhost:11435",
            system=self._get_super_strict_system_prompt(),
            temperature=0.7
        )
        logger.info(f"å·²è¿æ¥åˆ°Ollamaæ¨¡å‹: {model_name}")
        
        # åˆå§‹åŒ–RAGç³»ç»Ÿ
        self._init_rag_system()
        
        # åˆå§‹åŒ–å·¥å…·å­—å…¸
        self._init_tools_dict()
        
        logger.info("âœ… Aura Agentä¿®å¤ç‰ˆåˆå§‹åŒ–å®Œæˆ")
    
    def _get_super_strict_system_prompt(self) -> str:
        """è·å–è¶…çº§ä¸¥æ ¼çš„ç³»ç»Ÿæç¤ºè¯ï¼Œå½»åº•ç¦æ­¢thinkæ ‡ç­¾"""
        return """ä½ æ˜¯Auraï¼Œç”¨æˆ·çš„AIåŠ©æ‰‹ã€‚

ğŸš« ç»å¯¹ç¦æ­¢è§„åˆ™ ğŸš«
NEVER EVER use any of these in your response:
- <think> tags
- <thinking> tags  
- <reasoning> tags
- <analysis> tags
- ANY XML-style tags
- æ€è€ƒè¿‡ç¨‹æ ‡è®°
- å†…å¿ƒç‹¬ç™½

ğŸ¯ è¾“å‡ºè¦æ±‚ ğŸ¯
1. ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜
2. ä¸è¦æ˜¾ç¤ºä»»ä½•æ€è€ƒè¿‡ç¨‹
3. ä¸è¦ä½¿ç”¨ä»»ä½•æ ‡ç­¾
4. ä¿æŒç®€æ´å‹å¥½
5. å¦‚æœéœ€è¦å·¥å…·å¸®åŠ©ï¼Œæˆ‘ä¼šå•ç‹¬è°ƒç”¨

ä½ çš„èº«ä»½ï¼šæ™ºèƒ½åŠ©æ‰‹Auraï¼Œèƒ½å¤Ÿè®°å¿†ä¿¡æ¯ã€ä½¿ç”¨å·¥å…·ã€æä¾›å¸®åŠ©ã€‚

é‡è¦ï¼šä½ çš„å›ç­”åº”è¯¥ç›´æ¥å¼€å§‹ï¼Œä¸è¦æœ‰ä»»ä½•å‰ç¼€æˆ–æ ‡è®°ã€‚"""
    
    def _init_rag_system(self):
        """åˆå§‹åŒ–RAGçŸ¥è¯†æ£€ç´¢ç³»ç»Ÿ"""
        if not os.path.exists("db"):
            os.makedirs("db")
            
        self.rag_system = RAGSystem(persist_directory="db")
        logger.info("RAGç³»ç»Ÿå·²åˆå§‹åŒ–")
        
        self.retrieval_qa = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.rag_system.vectorstore.as_retriever(),
            return_source_documents=True
        )
    
    def _init_tools_dict(self):
        """åˆå§‹åŒ–å·¥å…·å­—å…¸"""
        self.tools_dict = {
            "search_web": tools.search_web,
            "read_file": tools.read_file,
            "write_file": tools.write_file,
            "list_directory": tools.list_directory,
            "search_knowledge": self.search_knowledge,
            "remember_fact": self.remember_fact,
            "recall_fact": self.recall_fact
        }
        logger.info(f"å·²åˆå§‹åŒ– {len(self.tools_dict)} ä¸ªå·¥å…·")
    
    def search_knowledge(self, query: str) -> str:
        """æœç´¢æœ¬åœ°çŸ¥è¯†åº“"""
        try:
            logger.info(f"æ­£åœ¨æœç´¢çŸ¥è¯†åº“: {query}")
            result = self.retrieval_qa({"query": query})
            return result["result"]
        except Exception as e:
            logger.error(f"çŸ¥è¯†åº“æœç´¢é”™è¯¯: {str(e)}")
            return f"çŸ¥è¯†åº“æœç´¢å‡ºé”™: {str(e)}"
    
    def remember_fact(self, fact_str: str) -> str:
        """è®°ä½ä¸€ä¸ªäº‹å®"""
        try:
            parts = fact_str.split('/')
            if len(parts) != 3:
                return "æ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨: category/key/value"
            
            category, key, value = parts
            self.long_term_memory.add_fact(category, key, value)
            logger.info(f"è®°å¿†å·²æ·»åŠ : {category}/{key}/{value}")
            return f"å·²è®°ä½: {category}/{key}/{value}"
        except Exception as e:
            logger.error(f"æ·»åŠ è®°å¿†é”™è¯¯: {str(e)}")
            return f"è®°å¿†å‡ºé”™: {str(e)}"
    
    def recall_fact(self, fact_str: str) -> str:
        """å›å¿†ä¸€ä¸ªäº‹å®"""
        try:
            parts = fact_str.split('/')
            if len(parts) != 2:
                return "æ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨: category/key"
                
            category, key = parts
            value = self.long_term_memory.get_fact(category, key)
            
            if value is None:
                return f"æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è®°å¿†: {category}/{key}"
                
            logger.info(f"è®°å¿†å·²æ£€ç´¢: {category}/{key}")
            return f"{category}/{key}: {value}"
        except Exception as e:
            logger.error(f"æ£€ç´¢è®°å¿†é”™è¯¯: {str(e)}")
            return f"å›å¿†å‡ºé”™: {str(e)}"
    
    def _should_use_tool(self, query: str) -> tuple[bool, str]:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦ä½¿ç”¨å·¥å…·"""
        query_lower = query.lower()
        
        # å®æ—¶ä¿¡æ¯æŸ¥è¯¢
        realtime_keywords = ["å¤©æ°”", "weather", "æ–°é—»", "news", "è‚¡ç¥¨", "ä»Šå¤©", "ç°åœ¨", "æœ€æ–°"]
        if any(keyword in query_lower for keyword in realtime_keywords):
            return True, "search_web"
        
        # çŸ¥è¯†åº“æŸ¥è¯¢
        knowledge_keywords = ["ä»€ä¹ˆæ˜¯", "è§£é‡Š", "åŸç†", "å¦‚ä½•", "æŠ€æœ¯", "ç®—æ³•", "æ¨¡å‹"]
        if any(keyword in query_lower for keyword in knowledge_keywords):
            return True, "search_knowledge"
        
        # è®°å¿†æ“ä½œ
        if "è®°ä½" in query or "remember" in query_lower:
            return True, "remember_fact"
        
        if "å›å¿†" in query or "è®°å¾—" in query or "recall" in query_lower:
            return True, "recall_fact"
        
        # æ–‡ä»¶æ“ä½œ
        if "è¯»å–æ–‡ä»¶" in query or "read file" in query_lower:
            return True, "read_file"
        
        if "å†™å…¥æ–‡ä»¶" in query or "write file" in query_lower:
            return True, "write_file"
        
        return False, ""
    
    def _extract_tool_input(self, query: str, tool_name: str) -> str:
        """ä»æŸ¥è¯¢ä¸­æå–å·¥å…·è¾“å…¥"""
        if tool_name == "search_web":
            return query
        elif tool_name == "search_knowledge":
            return query
        elif tool_name == "remember_fact":
            if "è®°ä½" in query:
                parts = query.split("è®°ä½")
                if len(parts) > 1:
                    content = parts[1].strip()
                    return f"user/preference/{content}"
            return query
        elif tool_name == "recall_fact":
            return "user/preference"
        
        return query
    
    def process_query(self, query: str) -> str:
        """å¤„ç†ç”¨æˆ·æŸ¥è¯¢ - ä¿®å¤ç‰ˆ"""
        try:
            logger.info(f"å¤„ç†ç”¨æˆ·æŸ¥è¯¢: {query}")
            
            # é¦–å…ˆåˆ¤æ–­æ˜¯å¦éœ€è¦å·¥å…·
            need_tool, tool_name = self._should_use_tool(query)
            
            if need_tool and tool_name in self.tools_dict:
                logger.info(f"ä½¿ç”¨å·¥å…·: {tool_name}")
                try:
                    tool_input = self._extract_tool_input(query, tool_name)
                    tool_result = self.tools_dict[tool_name](tool_input)
                    
                    # åŸºäºå·¥å…·ç»“æœç”Ÿæˆå›ç­”
                    context_prompt = f"""ç”¨æˆ·é—®é¢˜ï¼š{query}

å·¥å…·æŸ¥è¯¢ç»“æœï¼š{tool_result}

è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜ã€‚é‡è¦ï¼šä¸è¦ä½¿ç”¨ä»»ä½•XMLæ ‡ç­¾ï¼Œç›´æ¥ç»™å‡ºç­”æ¡ˆï¼š"""
                    
                    response = self.llm.invoke(context_prompt)
                    
                except Exception as tool_error:
                    logger.error(f"å·¥å…·è°ƒç”¨é”™è¯¯: {tool_error}")
                    # å¦‚æœå·¥å…·å¤±è´¥ï¼Œç›´æ¥ç”¨LLMå›ç­”
                    simple_prompt = f"è¯·ç›´æ¥å›ç­”ï¼š{query}"
                    response = self.llm.invoke(simple_prompt)
            else:
                # ç›´æ¥ç”¨LLMå›ç­”ï¼Œä½¿ç”¨è¶…å¼ºåˆ¶æç¤º
                simple_prompt = f"""è¯·ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œä¸è¦ä½¿ç”¨ä»»ä½•æ ‡ç­¾æˆ–æ€è€ƒè¿‡ç¨‹ï¼š

ç”¨æˆ·é—®é¢˜ï¼š{query}

ä½ çš„å›ç­”ï¼š"""
                response = self.llm.invoke(simple_prompt)
            
            # é¢å¤–çš„å®‰å…¨æ¸…ç†
            response = self._final_safety_clean(response)
            
            # ä¿å­˜å¯¹è¯å†å²
            self.long_term_memory.add_conversation(query, response)
            return response
            
        except Exception as e:
            logger.error(f"å¤„ç†æŸ¥è¯¢å‡ºé”™: {str(e)}")
            return "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶é‡åˆ°äº†é”™è¯¯ã€‚è¯·å°è¯•é‡æ–°è¡¨è¿°æ‚¨çš„é—®é¢˜ã€‚"
    
    def _final_safety_clean(self, response: str) -> str:
        """æœ€ç»ˆå®‰å…¨æ¸…ç†ï¼Œç¡®ä¿ç»æ— thinkæ ‡ç­¾"""
        # å¦‚æœå“åº”ä¸­åŒ…å«ä»»ä½•thinkç›¸å…³å†…å®¹ï¼Œå½»åº•æ¸…ç†
        if any(marker in response.lower() for marker in ['<think', 'think>', '<reasoning', 'reasoning>']):
            logger.warning("æ£€æµ‹åˆ°thinkæ ‡ç­¾ï¼Œæ‰§è¡Œç´§æ€¥æ¸…ç†")
            
            # æš´åŠ›æ¸…ç†ï¼šåˆ†è¡Œå¤„ç†ï¼Œåªä¿ç•™æ²¡æœ‰æ ‡ç­¾çš„è¡Œ
            lines = response.split('\n')
            clean_lines = []
            
            for line in lines:
                if not any(marker in line.lower() for marker in ['<think', 'think>', '<reasoning', 'reasoning>', 'thought:']):
                    clean_lines.append(line.strip())
            
            response = '\n'.join(clean_lines).strip()
            
            # å¦‚æœå…¨éƒ¨è¢«æ¸…ç†æ‰äº†ï¼Œè¿”å›é»˜è®¤å›ç­”
            if not response:
                response = "æˆ‘æ˜¯Auraï¼Œæ‚¨çš„AIåŠ©æ‰‹ã€‚æˆ‘èƒ½å¸®åŠ©æ‚¨è§£ç­”é—®é¢˜ã€å¤„ç†ä¿¡æ¯å’Œå®Œæˆå„ç§ä»»åŠ¡ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®æ‚¨çš„å—ï¼Ÿ"
        
        return response
    
    def load_knowledge(self, extension=".md") -> str:
        """åŠ è½½çŸ¥è¯†åˆ°RAGç³»ç»Ÿ"""
        try:
            data_dir = os.path.join(os.getcwd(), "data")
            logger.info(f"ä»{data_dir}åŠ è½½{extension}æ ¼å¼çš„çŸ¥è¯†æ–‡æ¡£")
            
            if not os.path.exists(data_dir):
                return f"âŒ dataç›®å½•ä¸å­˜åœ¨: {data_dir}"
                
            files = os.listdir(data_dir)
            target_files = [f for f in files if f.endswith(extension)]
            
            if not target_files:
                return f"âŒ dataç›®å½•ä¸­æ²¡æœ‰{extension}æ ¼å¼çš„æ–‡ä»¶"
            
            # æ‰‹åŠ¨åŠ è½½æ¯ä¸ªç›®æ ‡æ–‡ä»¶
            documents = []
            for file_name in target_files:
                file_path = os.path.join(data_dir, file_name)
                logger.info(f"åŠ è½½æ–‡ä»¶: {file_path}")
                
                try:
                    if extension == ".pdf":
                        from langchain.document_loaders import PyPDFLoader
                        loader = PyPDFLoader(file_path)
                    elif extension == ".csv":
                        from langchain.document_loaders import CSVLoader
                        loader = CSVLoader(file_path)
                    else:
                        from langchain.document_loaders import TextLoader
                        loader = TextLoader(file_path, encoding='utf-8')
                        
                    file_docs = loader.load()
                    documents.extend(file_docs)
                    logger.info(f"æˆåŠŸåŠ è½½ {file_path}, åŒ…å« {len(file_docs)} ä¸ªæ–‡æ¡£")
                except Exception as e:
                    logger.error(f"åŠ è½½æ–‡ä»¶ {file_path} å‡ºé”™: {str(e)}")
            
            if not documents:
                return "âŒ æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ–‡æ¡£"
            
            # æ–‡æœ¬åˆ†å‰²
            from langchain.text_splitter import RecursiveCharacterTextSplitter
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=500,
                chunk_overlap=50
            )
            splits = text_splitter.split_documents(documents)
            logger.info(f"åˆ›å»ºäº† {len(splits)} ä¸ªæ–‡æœ¬å—")
            
            # æ·»åŠ åˆ°å‘é‡åº“
            self.rag_system.vectorstore.add_documents(splits)
            logger.info("æ–‡æ¡£å·²æ·»åŠ åˆ°çŸ¥è¯†åº“")
            
            # æŒä¹…åŒ–ä¿å­˜
            self.rag_system.vectorstore.persist()
            logger.info("çŸ¥è¯†åº“å·²æŒä¹…åŒ–ä¿å­˜")
            
            return f"âœ… çŸ¥è¯†åº“å·²æ›´æ–°ï¼ŒåŠ è½½äº† {len(target_files)} ä¸ªæ–‡ä»¶ï¼Œå…± {len(splits)} ä¸ªæ–‡æœ¬å—"
        except Exception as e:
            logger.error(f"åŠ è½½çŸ¥è¯†å‡ºé”™: {str(e)}")
            return f"âŒ åŠ è½½çŸ¥è¯†å‡ºé”™: {str(e)}"
    
    def run_cli(self):
        """è¿è¡Œå‘½ä»¤è¡Œç•Œé¢"""
        print("=" * 60)
        print("âœ¨ Auraå·²å¯åŠ¨ (ä¿®å¤ç‰ˆ) - Thinkæ ‡ç­¾é—®é¢˜å·²è§£å†³")
        print(f"ğŸ¤– æ¨¡å‹: {self.model_name}")
        print("ğŸ’¡ æç¤º: ä½¿ç”¨'exit'æˆ–'é€€å‡º'å¯ä»¥ç»“æŸå¯¹è¯")
        print("ğŸ’¡ ç‰¹æ®Šå‘½ä»¤: 'åŠ è½½çŸ¥è¯†' - åŠ è½½dataç›®å½•ä¸­çš„æ–‡æ¡£åˆ°çŸ¥è¯†åº“")
        print("=" * 60)
        
        while True:
            user_input = input("\nğŸ‘¤ è¾“å…¥: ")
            
            # é€€å‡ºå‘½ä»¤
            if user_input.lower() in ["exit", "quit", "é€€å‡º"]:
                print("ğŸ‘‹ Auraæ­£åœ¨å…³é—­...")
                break
            
            # ç‰¹æ®Šå‘½ä»¤å¤„ç†
            if user_input.lower() == "åŠ è½½çŸ¥è¯†":
                extension = input("è¯·è¾“å…¥æ–‡ä»¶æ‰©å±•å(é»˜è®¤.md): ") or ".md"
                result = self.load_knowledge(extension)
                print(result)
                continue
                
            # å¤„ç†ä¸€èˆ¬æŸ¥è¯¢
            response = self.process_query(user_input)
            print(f"\nğŸ¤– Aura: {response}")

def main():
    """ä¸»ç¨‹åºå…¥å£"""
    try:
        # åˆ›å»ºä¿®å¤ç‰ˆAura Agent
        aura = AuraAgentFixed(model_name="qwen3:4b", verbose=False)
        
        # è¿è¡Œå‘½ä»¤è¡Œç•Œé¢
        aura.run_cli()
        
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ç¨‹åºå·²é€€å‡º")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå¯åŠ¨å¤±è´¥: {str(e)}")
        print("è¯·æ£€æŸ¥:")
        print("1. OllamaæœåŠ¡æ˜¯å¦æ­£åœ¨è¿è¡Œ")
        print("2. qwen3:4bæ¨¡å‹æ˜¯å¦å·²ä¸‹è½½")

if __name__ == "__main__":
    main()